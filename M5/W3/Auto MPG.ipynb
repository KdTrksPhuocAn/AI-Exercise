{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Download dataset\n",
        "!gdown --id 1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rYlLA0DSntb0",
        "outputId": "6498f274-294d-485d-bd31-6337b9fe018e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu\n",
            "To: /content/Auto_MPG_data.csv\n",
            "100% 15.4k/15.4k [00:00<00:00, 29.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "v2fMHFwWpQBa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Cài đặt thiết bị tính toán\n",
        "random_state = 59\n",
        "np.random.seed(random_state)\n",
        "torch.manual_seed(random_state)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_state)"
      ],
      "metadata": {
        "id": "1mfVnx7Lpzs6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ut3an1xlpwdU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Read dataset\n",
        "dataset_path = '/content/Auto_MPG_data.csv'\n",
        "dataset = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "W56ultPZpvCX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (a) Tách đặc trưng X và nhãn y\n",
        "X = dataset.drop(columns='MPG').values\n",
        "y = dataset['MPG'].values\n",
        "\n",
        "# (b) Chia dữ liệu train/val/test\n",
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "is_shuffle = True\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=val_size, random_state=random_state, shuffle=is_shuffle)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train, test_size=test_size, random_state=random_state, shuffle=is_shuffle)\n",
        "\n",
        "# (c) Chuẩn hóa dữ liệu\n",
        "normalizer = StandardScaler()\n",
        "X_train = normalizer.fit_transform(X_train)\n",
        "X_val = normalizer.transform(X_val)\n",
        "X_test = normalizer.transform(X_test)\n",
        "\n",
        "# Chuyển đổi sang tensor\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "xA56qohPptRG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 32\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "vukVpDrXpp2W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Build MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
        "        self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = F.relu(x)\n",
        "        out = self.output(x)\n",
        "        return out.squeeze(1)\n",
        "\n",
        "input_dims = X_train.shape[1]\n",
        "output_dims = 1\n",
        "hidden_dims = 64\n",
        "\n",
        "model = MLP(input_dims=input_dims,\n",
        "            hidden_dims=hidden_dims,\n",
        "            output_dims=output_dims).to(device)"
      ],
      "metadata": {
        "id": "mDKJDCY_pkmE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "F4U3vyH9phtX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def r_squared(y_true, y_pred):\n",
        "    y_true = torch.Tensor(y_true).to(device)\n",
        "    y_pred = torch.Tensor(y_pred).to(device)\n",
        "    mean_true = torch.mean(y_true)\n",
        "    ss_tot = torch.sum((y_true - mean_true) ** 2)\n",
        "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "4mH8BPYnpgPD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "train_losses, val_losses = [], []\n",
        "train_r2, val_r2 = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, val_loss = 0.0, 0.0\n",
        "    train_target, train_predict = [], []\n",
        "    val_target, val_predict = [], []\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for X_samples, y_samples in train_loader:\n",
        "        X_samples, y_samples = X_samples.to(device), y_samples.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_samples)\n",
        "        train_predict += outputs.tolist()\n",
        "        train_target += y_samples.tolist()\n",
        "        loss = criterion(outputs, y_samples)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    train_r2.append(r_squared(train_target, train_predict))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_samples, y_samples in val_loader:\n",
        "            X_samples, y_samples = X_samples.to(device), y_samples.to(device)\n",
        "            outputs = model(X_samples)\n",
        "            val_predict += outputs.tolist()\n",
        "            val_target += y_samples.tolist()\n",
        "            loss = criterion(outputs, y_samples)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_r2.append(r_squared(val_target, val_predict))\n",
        "\n",
        "    print(f'EPOCH {epoch + 1}: Training loss: {train_loss:.3f}, Validation loss: {val_loss:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzb3EH91pdeN",
        "outputId": "fa8662c3-ed49-4be0-81bf-049c16215cc4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1: Training loss: 282.769, Validation loss: 88.672\n",
            "EPOCH 2: Training loss: 137.669, Validation loss: 72.346\n",
            "EPOCH 3: Training loss: 71.007, Validation loss: 19.143\n",
            "EPOCH 4: Training loss: 25.083, Validation loss: 196.176\n",
            "EPOCH 5: Training loss: 96.139, Validation loss: 20.444\n",
            "EPOCH 6: Training loss: 17.765, Validation loss: 9.444\n",
            "EPOCH 7: Training loss: 18.486, Validation loss: 14.535\n",
            "EPOCH 8: Training loss: 37.859, Validation loss: 37.427\n",
            "EPOCH 9: Training loss: 17.133, Validation loss: 38.134\n",
            "EPOCH 10: Training loss: 22.991, Validation loss: 41.183\n",
            "EPOCH 11: Training loss: 26.723, Validation loss: 20.063\n",
            "EPOCH 12: Training loss: 9.852, Validation loss: 5.594\n",
            "EPOCH 13: Training loss: 15.143, Validation loss: 16.025\n",
            "EPOCH 14: Training loss: 12.213, Validation loss: 12.023\n",
            "EPOCH 15: Training loss: 14.222, Validation loss: 7.731\n",
            "EPOCH 16: Training loss: 10.845, Validation loss: 18.904\n",
            "EPOCH 17: Training loss: 12.312, Validation loss: 14.885\n",
            "EPOCH 18: Training loss: 15.474, Validation loss: 12.354\n",
            "EPOCH 19: Training loss: 13.783, Validation loss: 5.380\n",
            "EPOCH 20: Training loss: 7.285, Validation loss: 5.100\n",
            "EPOCH 21: Training loss: 8.168, Validation loss: 4.832\n",
            "EPOCH 22: Training loss: 6.277, Validation loss: 5.236\n",
            "EPOCH 23: Training loss: 10.050, Validation loss: 50.349\n",
            "EPOCH 24: Training loss: 10.607, Validation loss: 15.387\n",
            "EPOCH 25: Training loss: 9.235, Validation loss: 8.590\n",
            "EPOCH 26: Training loss: 9.764, Validation loss: 6.845\n",
            "EPOCH 27: Training loss: 6.272, Validation loss: 6.215\n",
            "EPOCH 28: Training loss: 10.287, Validation loss: 5.041\n",
            "EPOCH 29: Training loss: 6.383, Validation loss: 11.531\n",
            "EPOCH 30: Training loss: 17.039, Validation loss: 4.781\n",
            "EPOCH 31: Training loss: 7.879, Validation loss: 5.786\n",
            "EPOCH 32: Training loss: 6.630, Validation loss: 6.206\n",
            "EPOCH 33: Training loss: 7.156, Validation loss: 4.626\n",
            "EPOCH 34: Training loss: 6.758, Validation loss: 6.483\n",
            "EPOCH 35: Training loss: 7.450, Validation loss: 6.463\n",
            "EPOCH 36: Training loss: 5.831, Validation loss: 5.967\n",
            "EPOCH 37: Training loss: 5.947, Validation loss: 6.008\n",
            "EPOCH 38: Training loss: 7.273, Validation loss: 10.705\n",
            "EPOCH 39: Training loss: 7.611, Validation loss: 14.612\n",
            "EPOCH 40: Training loss: 7.478, Validation loss: 5.625\n",
            "EPOCH 41: Training loss: 5.840, Validation loss: 20.782\n",
            "EPOCH 42: Training loss: 8.010, Validation loss: 5.530\n",
            "EPOCH 43: Training loss: 7.096, Validation loss: 28.658\n",
            "EPOCH 44: Training loss: 10.015, Validation loss: 10.240\n",
            "EPOCH 45: Training loss: 6.435, Validation loss: 4.911\n",
            "EPOCH 46: Training loss: 8.238, Validation loss: 7.057\n",
            "EPOCH 47: Training loss: 6.315, Validation loss: 6.785\n",
            "EPOCH 48: Training loss: 5.323, Validation loss: 6.720\n",
            "EPOCH 49: Training loss: 6.652, Validation loss: 9.927\n",
            "EPOCH 50: Training loss: 6.583, Validation loss: 12.207\n",
            "EPOCH 51: Training loss: 8.147, Validation loss: 7.704\n",
            "EPOCH 52: Training loss: 10.966, Validation loss: 5.014\n",
            "EPOCH 53: Training loss: 7.193, Validation loss: 4.809\n",
            "EPOCH 54: Training loss: 6.586, Validation loss: 5.283\n",
            "EPOCH 55: Training loss: 5.428, Validation loss: 5.254\n",
            "EPOCH 56: Training loss: 5.410, Validation loss: 6.241\n",
            "EPOCH 57: Training loss: 7.397, Validation loss: 7.304\n",
            "EPOCH 58: Training loss: 6.877, Validation loss: 19.924\n",
            "EPOCH 59: Training loss: 7.607, Validation loss: 6.910\n",
            "EPOCH 60: Training loss: 5.577, Validation loss: 4.655\n",
            "EPOCH 61: Training loss: 5.597, Validation loss: 9.228\n",
            "EPOCH 62: Training loss: 11.281, Validation loss: 4.500\n",
            "EPOCH 63: Training loss: 6.602, Validation loss: 13.699\n",
            "EPOCH 64: Training loss: 6.271, Validation loss: 6.322\n",
            "EPOCH 65: Training loss: 8.267, Validation loss: 5.209\n",
            "EPOCH 66: Training loss: 5.305, Validation loss: 5.244\n",
            "EPOCH 67: Training loss: 5.974, Validation loss: 6.119\n",
            "EPOCH 68: Training loss: 10.930, Validation loss: 7.538\n",
            "EPOCH 69: Training loss: 7.000, Validation loss: 9.098\n",
            "EPOCH 70: Training loss: 6.188, Validation loss: 5.058\n",
            "EPOCH 71: Training loss: 5.184, Validation loss: 4.528\n",
            "EPOCH 72: Training loss: 5.927, Validation loss: 9.171\n",
            "EPOCH 73: Training loss: 6.805, Validation loss: 9.461\n",
            "EPOCH 74: Training loss: 6.086, Validation loss: 5.240\n",
            "EPOCH 75: Training loss: 5.313, Validation loss: 16.254\n",
            "EPOCH 76: Training loss: 7.395, Validation loss: 18.961\n",
            "EPOCH 77: Training loss: 10.982, Validation loss: 7.603\n",
            "EPOCH 78: Training loss: 6.881, Validation loss: 5.415\n",
            "EPOCH 79: Training loss: 5.829, Validation loss: 10.532\n",
            "EPOCH 80: Training loss: 8.031, Validation loss: 5.808\n",
            "EPOCH 81: Training loss: 10.559, Validation loss: 5.661\n",
            "EPOCH 82: Training loss: 5.536, Validation loss: 5.436\n",
            "EPOCH 83: Training loss: 5.733, Validation loss: 8.689\n",
            "EPOCH 84: Training loss: 5.447, Validation loss: 5.063\n",
            "EPOCH 85: Training loss: 4.924, Validation loss: 6.608\n",
            "EPOCH 86: Training loss: 4.733, Validation loss: 6.193\n",
            "EPOCH 87: Training loss: 5.094, Validation loss: 7.785\n",
            "EPOCH 88: Training loss: 5.322, Validation loss: 4.985\n",
            "EPOCH 89: Training loss: 4.837, Validation loss: 5.246\n",
            "EPOCH 90: Training loss: 6.566, Validation loss: 5.694\n",
            "EPOCH 91: Training loss: 5.474, Validation loss: 8.742\n",
            "EPOCH 92: Training loss: 7.184, Validation loss: 5.376\n",
            "EPOCH 93: Training loss: 6.221, Validation loss: 5.282\n",
            "EPOCH 94: Training loss: 9.373, Validation loss: 7.654\n",
            "EPOCH 95: Training loss: 6.485, Validation loss: 5.797\n",
            "EPOCH 96: Training loss: 8.487, Validation loss: 22.967\n",
            "EPOCH 97: Training loss: 7.113, Validation loss: 11.046\n",
            "EPOCH 98: Training loss: 7.269, Validation loss: 7.265\n",
            "EPOCH 99: Training loss: 5.988, Validation loss: 5.091\n",
            "EPOCH 100: Training loss: 6.082, Validation loss: 4.776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_hat = model(X_test)\n",
        "    test_set_r2 = r_squared(y_hat, y_test)\n",
        "    print('Evaluation on test set:')\n",
        "    print(f'R2: {test_set_r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DscPQCJpcRT",
        "outputId": "9e99d752-4c12-414a-f6a7-38b48421b2d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test set:\n",
            "R2: 0.8417800068855286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_DMkJvbp2go"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}